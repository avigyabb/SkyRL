# Biomni CodeAct with LLM-based rubric rewards (RL training config for Qwen3-30B-A3B)
# Uses BiomniCodeActRubricAgent which evaluates trajectories using:
# - gt_reward: ground truth from task.reward()
# - rubric_reward: LLM critic evaluation (max 5, normalized from 50)
# - ft_reward: format validation (max 1)
# Total reward = gt_reward + rubric_reward + ft_reward (max 7)

agent_cls: skyrl_agent.agents.biomni_codeact.BiomniCodeActRubricAgent

# Overlong rollout filter: mask out loss for rollouts that are both
# 1. Longer than overlong_filter_threshold tokens, AND
# 2. Have format reward (ft_reward) == 0
# This filters degenerated cases where agent produces very long non-compliant outputs
overlong_filter_enabled: true
overlong_filter_threshold: 40000

# Heavy logging: log detailed trajectory samples periodically
use_log_heavy: true
log_heavy_freq: 8

task: skyrl_agent.tasks.general_react.utils.DummyReactTask
tools: []

data:
  data_source_key: data_source

generator:
  infer_backend: skyrl-train
  backend_config:
    model_name: Qwen/Qwen3-30B-A3B-Thinking-2507
    api_url: http://localhost:30000

  num_trajectories: 8
  max_iterations: 50
  # Keep prompt budget aligned with trainer.max_prompt_length & vLLM max_model_len.
  max_prompt_length: 49152
  sampling_params:
    temperature: 1.0
    top_p: 1.0
    # This is the agent-side generation cap. The skyrl-train generator.max_generate_length
    # should match it.
    max_tokens: 4096
    # Enable returning logprobs for the chosen token (for TIS)
    logprobs: 0
  remove_think_tokens: false
  vision_is_active: false
  qwen3_enable_thinking: true
  qwen3_acc_thinking: true

dispatcher:
  type: async_batch
  scheduler: naive
  max_parallel_agents: 128
  max_eval_parallel_agents: 128
