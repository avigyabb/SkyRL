# Biomni CodeAct with LLM-based rubric rewards (RL training config)
# Uses BiomniCodeActRubricAgent which evaluates trajectories using:
# - gt_reward: ground truth from task.reward()
# - rubric_reward: LLM critic evaluation (max 5, normalized from 50)
# - ft_reward: format validation (max 1)
# Total reward = gt_reward + rubric_reward + ft_reward (max 7)

agent_cls: skyrl_agent.agents.biomni_codeact.BiomniCodeActRubricAgent

# Overlong rollout filter: mask out loss for rollouts that are both
# 1. Longer than overlong_filter_threshold tokens, AND
# 2. Have format reward (ft_reward) == 0
# This filters degenerated cases where agent produces very long non-compliant outputs
overlong_filter_enabled: true
overlong_filter_threshold: 32768  # 32k tokens

# Heavy logging: log detailed trajectory samples periodically
use_log_heavy: true
log_heavy_freq: 8  # Log every 8 steps

task: skyrl_agent.tasks.general_react.utils.DummyReactTask

tools: []

data:
  data_source_key: data_source

generator:
  infer_backend: skyrl-train
  backend_config:
    model_name: Qwen/Qwen3-8B
    api_url: http://localhost:30000

  num_trajectories: 8
  max_iterations: 48
  max_prompt_length: 45056
  sampling_params:
    temperature: 1.0
    top_p: 1.0
    max_tokens: 4096
  remove_think_tokens: false
  vision_is_active: false
  qwen3_enable_thinking: true
  qwen3_acc_thinking: true

dispatcher:
  type: async_batch
  scheduler: naive
  max_parallel_agents: 128
  max_eval_parallel_agents: 128
